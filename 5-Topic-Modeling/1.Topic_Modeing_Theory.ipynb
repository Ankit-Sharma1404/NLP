{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modeling**\n",
    "<ul>\n",
    "    <li>It allows for us to efficiently analyze large volumes of text by clustering documents into topics.</li>\n",
    "    <li>A large amount of text data is <b>unlabeled</b> meaning we won't be able to apply our previous supervised learning approaches to create machine learning models for tha data.</li>\n",
    "    <li>If we have <b>unlabeled</b> data, then we can attempt to \"discover\" labels.</li>\n",
    "    <li>In the case of text data, this means attempting to discover clusters of documents, grouped together by topic.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation**\n",
    "<ul>\n",
    "    <li>Johann Peter Gustav Lejeune Dirichlet was a German mathematician in the 1800s who contributes widely to the field of modern mathematics.</li>\n",
    "    <li>There is a probability distribution named after him \"Dirichlet Distribution\".</li>\n",
    "    <li>Latent Dirichlet Allocation is based off this probability distribution.</li>\n",
    "    <li>In 2003 LDA was first published as a graphical model for topic discovery in Journal of Machine Learning Research by David Blei, Andrew Ng and Michael I. Jordan.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Assumptions of <b>LDA</b> for Topic Modeling</li>\n",
    "    <ul>\n",
    "        <li>Documents with similar topics use similar groups of words.</li>\n",
    "        <li>Latent topics can then found by searching for groups of words that frequently occur together in documents across the corpus.</li>\n",
    "        <li>Documents are probability distributions over latent topics.</li>\n",
    "        <li>Topics themselves are probability distributions over words.</li>\n",
    "        <li>Documents are probability distributions over latent topics.\n",
    "            <br><font color='blue'>Diagram in Notebook</font></li>\n",
    "        <li>Topics themeselves are probability distributions over words.\n",
    "            <br><font color='blue'>Diagram in Notebok</font></li>\n",
    "    </ul>\n",
    "    <li><b>LDA</b> represents documents as mixtures of topics that split out words with certain probabilities.</li>\n",
    "    <li>It assumes that documents are produced in the following fashion:</li>\n",
    "    <ul>\n",
    "        <li>Choose a topic mixture for the document (according to a Dirichlet distribution over a fixed set of K topics).</li>\n",
    "        <li>e.g. 60% business, 20% politics, 10% food.</li>\n",
    "    </ul>\n",
    "    <li>Generate each word in the document by:</li>\n",
    "    <ul>\n",
    "        <li>First picking a topic according to the multinomail distribution that we sampled previously (60% business, 20% politics, 10% food).</li>\n",
    "        <li>Using the topic to generate the word itself (according to the topic's multinomial distribution).</li>\n",
    "        <li><b>For Example,</b> if we selected the food topic, we might generate the word \"apple\" with 60% probability, \"home\" with 30% probability, and so on.</li>\n",
    "    </ul>\n",
    "    <li>Assuming this generative model for a collection of documents, <b>LDA</b> then tries to backtrace form the documents to find a set of topics that are likely to have generated the collection.</li>\n",
    "    <li>It assumes that documents are produced in the following fashion:</li>\n",
    "    <ul>\n",
    "        <li>Choose a topic mixture for the document (according to a Dirichlet distribution over a fixed set of K topics).</li>\n",
    "        <li><b>e.g.</b> 60% business, 20% politics, 10% food.</li>\n",
    "    </ul>\n",
    "    <li>Now imagine we have a set of documents.</li>\n",
    "    <li>We've chosen some fixed number of K topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic.</li>\n",
    "    <li>Go through each document, and randomly assign each word in the document to one of the K topics.</li>\n",
    "    <li>This random assignment already gives us both topic representations of all the topics (Note, these initial random topics won't make sense).</li>\n",
    "    <li>Now we iterate over every word in every document to improve these topics.</li>\n",
    "    <li>For every word in every document and for each topic <b>t</b> we calculate:</li>\n",
    "    <ul>\n",
    "        <li>p(topic <b>t</b> | document <b>d</b>) = the proportion of words in document <b>d</b> that are currently assigned to topic <b>t</b></li>\n",
    "        <li>p(word <b>w</b> | topic <b>t</b>) = the proportion of words in document <b>t</b> that come from this word <b>w</b></li>\n",
    "    </ul>\n",
    "    <li>Reassign <b>w</b> a new topic, where we choose topic <b>t</b> with probability <b>p(topic t | document d) * p(word w | topic t)</b></li>\n",
    "    <li>This is essentially the probability that topic <b>t</b> generated word <b>w</b></li>\n",
    "    <li>After repeating the previous step a large number of times, we eventually reach a roughly state where the assignments are acceptable.</li>\n",
    "    <li>At the end we have each document assigned to a topic.</li>\n",
    "    <li>We also can search for the words that have the highest probability of being assigned to a topic.</li>\n",
    "    <li>We end up with an output such as:</li>\n",
    "    <ul>\n",
    "        <li>Document assigned to Topic #4</li>\n",
    "        <li>Most common words (highest probability) for Topic #4:</li>\n",
    "        <ul>\n",
    "            <li>['cat', 'vet', 'birds', 'dog', ... , 'home']</li>\n",
    "        </ul>\n",
    "        <li>It is up to ther user to interpret these topics.</li>\n",
    "    </ul>\n",
    "    <li>Two important notes:</li>\n",
    "    <ul>\n",
    "        <li>The user must decide on the amount of topics present in the document.</li>\n",
    "        <li>The user must interpret what the topics are.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
